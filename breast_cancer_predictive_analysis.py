# -*- coding: utf-8 -*-
"""Breast Cancer Predictive Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1huSuCe0eBPQa_2TblXMfKDOZk7liS3sO

# **Breast Cancer Datasets**
###### (Source: [Kaggle](https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset))

---

Breast cancer is the most common cancer amongst women in the world. It accounts for 25% of all cancer cases, and affected over 2.1 Million people in 2015 alone. It starts when cells in the breast begin to grow out of control. These cells usually form tumors that can be seen via X-ray or felt as lumps in the breast area.

The key challenges against itâ€™s detection is how to classify tumors into malignant (cancerous) or benign(non cancerous). We ask you to complete the analysis of classifying these tumors using machine learning (with SVMs) and the Breast Cancer Wisconsin (Diagnostic) Dataset. 

### Objective:
- Understand the Dataset & cleanup (if required).
- Build classification models to predict whether the cancer type is Malignant or Benign.
- Also fine-tune the hyperparameters & compare the evaluation metrics of various classification algorithms.

## Data Preparation

### Datasets:
The breast has 569 datasets that spread across 32 columns. Those columns are as follows
- `id`: Unique ID
- `diagnosis`: M - Malignant (Cancerous), B - Benign (Non-cancerous)
- `radius_mean`: Radius of Lobes
- `texture_mean`: Mean of Surface texture
- `perimeter_mean`: Outer Perimeter of Lobes
- `area_mean`: Mean Area of Lobes
- `smoothness_mean`: Mean of Smoothness Levels
- `compactness_mean`: Mean of Compactness
- `concavity_mean`: Mean of Concavity
- `concave points_mean`: Mean of concave points
- `symmetry_mean`: Mean of Symmetry
- `fractal_dimension_mean`: Mean of Fractal Dimension
- `radius_se`: SE of Radius
- `texture_se`: SE of Texture
- `perimeter_se`: Perimeter of SE
- `area_se`: Area of SE
- `smoothness_se`: SE of Smoothness
- `compactness_se`: SE of Compactness
- `concavity_se`: SE of Concavity
- `concave points_se`: SE of Concave Points
- `symmetry_se`: SE of Symmetry
- `fractal_dimension_se`: SE of Fractal Dimension
- `radius_worst`: Worst Radius
- `texture_worst`: Worst Texture
- `perimeter_worst`: Worst Perimeter
- `area_worst`: Worst Area
- `smoothness_worst`: Worst Smoothness
- `compactness_worst`: Worst Compactness
- `concavity_worst`: Worst Concavity
- `concave points_worst`: Worst Concave Points
- `symmetry_worst`: Worst Symmetry
- `fractal_dimension_worst`: Worst Fractal Dimension

#### Import Required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# %matplotlib inline

"""Download the Dataset from kaggle"""

url = "https://storage.googleapis.com/kagglesdsdata/datasets/1829286/2984728/breast-cancer.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220817%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220817T072622Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=06ff8ac1469c0f24f00cfe7b60fde837cc4cb4e0e891ef01c928fe2a8fd381aee579c3646a21c0003d886eef8c58c76eabe9cd4c4ef4d1367ed8040f5ae556c92795f6e22ec4f21db1666766ff31eca5e9588835ef2411b50febe571142beb6f2c7247f960ae5334c39648605d46b301f7a6672232aa056939adf04034b03696870ee3e818a0db3cb179754c8ec14e4a6770eb7bef245a2011324462c8af8b0d9c833f5c3e1ee2c1a40451f543d365a635c974cf7aaada12eb4beaf60ff6a9b3b21a7f1d4d51c3140073e0870dc4961a7bc02388c07385be07660d6c3a932fc7ba67256ea552f63a0cd8fb5888c6413e8f1e8bbd3b83c72cd39ad2f5e24fe040"
df = pd.read_csv(url)
df.head(5)

"""In here, the data will be enquired and displayed from the dataset that has been retrieved"""

# Print out the rows, columns and informations
print("The amount of data from the datasets are: {}".format(df.shape[0]))
print("The columns of the datasets are: {}".format(df.shape[1]))

# Print the column details
df.dtypes

"""Mostly of the columns are float64 data types with a binary classifier at the column `diagnosis`. That will be our label and the output that we want to classify, which are Malignant (M) or Benign (B).
The column `id` is not usable and can be dropped.
"""

df.drop(['id'], inplace=True, axis=1)
df.head()

"""For this practice, we will only consider the mean part of the columns (there are 30 columns, 10 should be enough for this practice), so the `se` and the `worst` column features can be omitted and only the mean part and the label `diagnosis` remains."""

df = df[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 
         'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
         'diagnosis']]
df.head()

"""Since the `diagnosis` column is a label and we want the result to be either M - Malignant (Cancerous), or B - Benign (Non-cancerous). So we transform the diagnosis column into **0** representing **B** (Not Canerous) and **1** representing **M** (Cancerous)."""

df['diagnosis'].replace(['M', 'B'], [1, 0], inplace=True)
df.head()

"""Now that there are 11 numerical features left, including the newly transformed `diagnosis` column, we can describe each of the data."""

# Describe the data 
df.describe()

"""Finally, we check if any of the data has null value"""

print('Data columns with null values: ', df.isnull().sum(), sep='\n')

"""None of the data has null value. That means our dataset is safe and now we are onto next step, which is Data Analysis.

## Data Analysis

In this section, we will analyze the data such as finding outliers and correlations.

Outliers is used to eliminate data that is way far away from the trend line. In this practice, we will observe every columns by using Seaborn Boxplot. We don't include `diagnosis` column as that is basically either 0 or 1.
"""

numerical_features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 
                      'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']

fig, axes = plt.subplots(4, 3, figsize=(20, 15))
fig.delaxes(axes[3, 1])
fig.delaxes(axes[3, 2])

for i in range(0, len(numerical_features)):
    sns.boxplot(ax=axes[i // 3, i % 3], x = df[numerical_features[i]])

"""There is some outliers on the columns, such as the `area_mean`, the `fractal_mean`, and the `compactness_mean`, and a bunch of those columns. We can remove those outliers by using the IQR Method.

$IQR = Q_3 - Q_1$

Upper Bound = $Q_3 + 1.5 \times IQR$

Lower Bound = $Q_1 - 1.5 \times IQR$
"""

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis = 1)]

df.shape

"""Now let's run the boxplot again and see if we have removed the outliers."""

fig, axes = plt.subplots(4, 3, figsize=(20, 15))
fig.delaxes(axes[3, 1])
fig.delaxes(axes[3, 2])

for i in range(0, len(numerical_features)):
    sns.boxplot(ax=axes[i // 3, i % 3], x = df[numerical_features[i]])

"""Although there are some noticable outliers, it is still better than before. Now we move on to plotting our data distribution.

Since all of our columns are using numerical features, that means we can directly plot every column to show the data distributions.
"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""The result is mostly in part with the boxplot that we have saw earlier on. But in this visualization, we can see the actual score on which is the highest in each column.

We can also identify the colleration between each columns using the seaborn pairplot.
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde', hue = 'diagnosis')

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title("Correlation Matrix untuk Fitur Numeric", size="20")

"""From the correlation chat we can infer that 
- the `texture_mean` has the least correlation among all columns
- `radius_mean`, `perimeter_mean`, and `area_mean` are highly correlated to each other
- `compactness_mean`, `concavity_mean`, and `concave points_mean` are highly correlated to each other.
- `concavity_mean` and `concave points_mean` are correlated with `radius_mean`
- `fractal_dimension_mean` is the most least correlated with `target`

Since `fractal_dimension_mean` is the least correlated with target`, this can be removed.
"""

df.drop(['fractal_dimension_mean'], inplace=True, axis=1)
df.head()

"""## Feature Engineering

If we can see, the columns `radius_mean`, `perimeter_mean`, and `area_mean` are correlated each other and score almost same in terms of the correlation with the `diagnosis` column. Hence, we can try to do a PCA implementation to reduce our dimension.
"""

sns.pairplot(df[['radius_mean','perimeter_mean','area_mean', 'diagnosis']], hue = 'diagnosis', plot_kws={"s": 3});

from sklearn.decomposition import PCA

pca = PCA(n_components = 3, random_state = 123)
pca.fit(df[['radius_mean', 'perimeter_mean', 'area_mean']])
transform = pca.transform(df[['radius_mean', 'perimeter_mean', 'area_mean']])

"""After implementing PCA on the `radius_mean`, `perimeter_mean`, and `area_mean`, we can find the proportion distribution among them."""

pca.explained_variance_ratio_.round(5)

"""We can say that the majority of the information has been described by the `radius_mean`. That means we can reduce our dimension of the 3 columns into one column `measurement_mean` as `perimter_mean` and `area_mean` technically can be derived from `radius_mean` by mathematics formula."""

pca = PCA(n_components = 1, random_state = 123)
pca.fit(df[['radius_mean', 'perimeter_mean', 'area_mean']])
df['measurement_mean'] = pca.transform(df.loc[:, ('radius_mean', 'perimeter_mean', 'area_mean')]).flatten()
df.drop(['radius_mean', 'perimeter_mean', 'area_mean'], inplace=True, axis=1)

df.head()

"""We now have reduced our dimension. Now we can move to splitting our data into Train and Test set with 80% of the data are train data."""

from sklearn.model_selection import train_test_split

x = df.drop(['diagnosis'], axis=1)
y = df['diagnosis']
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=123)

print(f"Total dataset: {len(x)}")
print(f"Total Train  : {len(x_train)}")
print(f"Total Test   : {len(x_test)}")

"""After splitting our dataset, it is time that we standardize all numerical features so that it is easier for the model to be trained and run."""

from sklearn.preprocessing import StandardScaler
 
numerical_features = ['texture_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 
                      'symmetry_mean', 'measurement_mean']
scaler = StandardScaler()
scaler.fit(x_train[numerical_features])
x_train[numerical_features] = scaler.transform(x_train.loc[:, numerical_features])
x_train[numerical_features].head()

"""Everything is standardized. We can describe the `numerical_features` and find that all of the columns are 0 in mean and 1 in standard deviation, with a little bit of bias/error."""

x_train[numerical_features].describe().round(4)

"""## Model Development

In this practice, we will do 3 algorithms to model our dataset, which is **K-Nearest Neighbor**, **Random Forest**, and **Boosting Algorithm**.

First, we prepare the dataframe to contain all algorithms that we are going to use.
"""

models = pd.DataFrame(index=['KNN', 'RandomForest', 'Boosting'],
                      columns=['train_acc', 'test_acc'])

"""#### K-Nearest Neighbor"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import accuracy_score

knn = KNeighborsRegressor(n_neighbors=10, metric = 'minkowski', p = 2)
knn.fit(x_train, y_train)

models.loc['KNN', 'train_acc'] = accuracy_score(y_pred=np.rint(knn.predict(x_train)), y_true=y_train)

"""#### Random Forest"""

from sklearn.ensemble import RandomForestRegressor

RF = RandomForestRegressor(n_estimators=20, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(x_train, y_train)

models.loc['RandomForest', 'train_acc'] = accuracy_score(y_pred=np.rint(RF.predict(x_train)), y_true=y_train)

"""#### Boosting Algorithm"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(x_train, y_train)
models.loc['Boosting', 'train_acc'] = accuracy_score(y_pred=np.rint(boosting.predict(x_train)), y_true=y_train)

"""## Evaluating Model

The modeling part is done. Now we want to evaluate the model by using the MSE formula that will calculate the 
"""

x_test.loc[:, numerical_features] = scaler.transform(x_test[numerical_features])

model_dict = {'KNN': knn, 'RandomForest': RF, 'Boosting': boosting}

for name, model in model_dict.items():
    models.loc[name, 'test_acc'] = accuracy_score(y_true=y_test, y_pred=np.rint(model.predict(x_test)))

models

"""From the evaluation, we can infer that Boosting has the best model among all three algorithms that we have tested, which has a train score of 97.76% in accuracy and a test score of 97% in accuracy.

We can visualize the result based on the value of the `test_acc` and see the difference between the value of `train_acc` and `test_acc`
"""

fig, ax = plt.subplots()
models.sort_values(by='test_acc', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Finally, we will test our models by evaluating the result of the models and comparing with the true value from the test set."""

prediction = x_test.iloc[:10].copy()
pred_dict = {'y_true': y_test[:10]}
for name, model in model_dict.items():
    pred_dict['prediction_' + name] = np.rint(model.predict(prediction))

pd.DataFrame(pred_dict)

"""We can infer that KNN and Boosting algorithm has 9/10 and RandomForest has 8/10 to the original value. From the sample, we can infer that this is as right as the accuracy result that has been gained from the evaluation model above, with the RF algorithm being the worst and the Boosting Algorithm being the best model in this practice."""